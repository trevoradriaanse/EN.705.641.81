<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Homework 1: Background Review + Building a classifier" />
  <title> 705.641.81: Natural Language Processing Self-Supervised Models </title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title"> 705.641.81: Natural Language Processing
Self-Supervised Models </h1>
<p class="author">Homework 1: Background Review + Building a
classifier</p>
<p class="date">For homework deadline and collaboration policy, please
see our Canvas page.<br />
Name: __________________________ <br />
Collaborators, if any:
____________________________________________________<br />
Sources used for your homework, if any:
_______________________________________ </p>
</header>
<p>This assignment combines knowledge and skills across several
disciplines. The purpose of this assignment is to make sure you are
prepared for this course. We anticipate that each of you will have
different strengths and weaknesses, so don’t be worried if you struggle
with <em>some</em> aspects of the assignment. But if you find this
assignment to be very difficult overall, that is an early warning sign
that you may not be prepared to take this course at this time.<br />
To succeed in the course, you will need to know or very quickly get up
to speed on:</p>
<ul>
<li><p>Math to the level of the course prerequisites: linear algebra,
multivariate calculus, some probability.</p></li>
<li><p>Statistics, algorithms, and data structures to the level of the
course prerequisites.</p></li>
<li><p>Python programming, and the ability to translate from math or
algorithms to programming and back.</p></li>
<li><p>Some basic LaTeX skills so that you can typeset equations and
submit your assignments.</p></li>
</ul>
<h1
id="refreshing-the-rows-and-columns-linear-algebra-review">Refreshing
the Rows and Columns: Linear Algebra Review</h1>
<p>For these questions, you may find it helpful to review <a
href="http://www.cs.ubc.ca/~schmidtm/Documents/2009_Notes_LinearAlgebra.pdf">these
notes on linear algebra</a>.</p>
<h2 id="basic-operations">Basic Operations</h2>
<p>Use the definitions below, <span class="math display">\[\alpha =
2,\quad
\mathbf{x}= \left[\begin{array}{c}
0\\
1\\
2\\
\end{array}\right], \quad
\mathbf{y}= \left[\begin{array}{c}
3\\
4\\
5\\
\end{array}\right],\quad
\textbf{z} = \left[\begin{array}{c}
1\\
2\\
-1\\
\end{array}\right],\quad
A = \left[\begin{array}{ccc}
3 &amp; 2 &amp; 2\\
1 &amp; 3 &amp; 1\\
1 &amp; 1 &amp; 3
\end{array}\right],\]</span> and use <span
class="math inline">\(x_i\)</span> to denote element <span
class="math inline">\(i\)</span> of vector <span
class="math inline">\(\mathbf{x}\)</span>. Evaluate the following
expressions:</p>
<ol>
<li><p><span class="math inline">\(\sum_{i=1}^n x_iy_i\)</span> (inner
product).</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n x_iz_i\)</span> (inner
product between orthogonal vectors).</p></li>
<li><p><span
class="math inline">\(\alpha(\mathbf{x}+\mathbf{y})\)</span> (vector
addition and scalar multiplication).</p></li>
<li><p><span class="math inline">\(\left\|\mathbf{x} \right\|\)</span>
(Euclidean norm of <span class="math inline">\(x\)</span>).</p></li>
<li><p><span class="math inline">\(\mathbf{x}^\top\)</span> (vector
transpose).</p></li>
<li><p><span class="math inline">\(A\mathbf{x}\)</span> (matrix-vector
multiplication).</p></li>
<li><p><span class="math inline">\(\mathbf{x}^\top A\mathbf{x}\)</span>
(quadratic form).</p></li>
</ol>
<p>Note, you do not need to show your work.</p>
<div class="wraptable">
<p><span>r</span><span>7.2cm</span></p>
<div class="framed">
<p>In the realm where self-supervision crafts,<br />
You’ll need some skills to climb its shafts.<br />
<br />
Firstly, you’ll need your linear algebra potion,<br />
Ready to solve with matrix motion.<br />
Eigenvalues, vectors, and spaces we’ll trek,<br />
<br />
Then we venture into probability’s domain,<br />
Where uncertainty and statistics maintain.<br />
Random variables, distributions so fair,<br />
<br />
Then you must program, in PyTorch, no less,<br />
Tensors, data loaders: your new-found friends,<br />
Backpropagation till the very end.<br />
—GPT-4 Jan 8 2024</p>
</div>
</div>
<h2 id="matrix-algebra-rules">Matrix Algebra Rules</h2>
<p>Assume that <span class="math inline">\(\left\lbrace \mathbf{x},
\mathbf{y}, \mathbf{z} \right\rbrace\)</span> are <span
class="math inline">\(n \times 1\)</span> column vectors and <span
class="math inline">\(\{A, B, C\}\)</span> are <span
class="math inline">\(n \times n\)</span> real-valued matrices, and
<span class="math inline">\(\mathbf{I}\)</span> is the identity matrix
of appropriate size. State whether each of the below is true in general
(you do not need to show your work).</p>
<ol>
<li><p><span class="math inline">\(\mathbf{x}^\top \mathbf{y}=
\sum_{i=1}^n x_iy_i\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf{x}^\top \mathbf{x}=
\left\|\mathbf{x} \right\|^2\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf{x}^\top \mathbf{x}=
\mathbf{x}\mathbf{x}^\top\)</span>.</p></li>
<li><p><span
class="math inline">\((\mathbf{x}-\mathbf{y})^\top(\mathbf{y}-\mathbf{x})
= \left\|\mathbf{x} \right\|^2 - 2\mathbf{x}^\top \mathbf{y}+
\left\|\mathbf{y} \right\|^2\)</span>.</p></li>
<li><p><span class="math inline">\(AB=BA\)</span>.</p></li>
<li><p><span class="math inline">\(A(B + C) = AB + AC\)</span>.</p></li>
<li><p><span class="math inline">\((AB)^\top = A^\top
B^\top\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf{x}^\top A\mathbf{y}=
\mathbf{y}^\top A^\top \mathbf{x}\)</span>.</p></li>
<li><p><span class="math inline">\(A^\top A = \mathbf{I}\)</span> is the
columns of <span class="math inline">\(A\)</span> are
orthonormal.</p></li>
</ol>
<h2 id="matrix-operations">Matrix operations</h2>
<p>Let <span class="math inline">\(\textbf{B} =
\begin{bmatrix}
1 &amp; -1 &amp; 0 \\  
-1 &amp; 2 &amp; -1 \\
0 &amp; -1 &amp; 1
\end{bmatrix}\)</span>.</p>
<ul>
<li><p>Is <span class="math inline">\(\textbf{B}\)</span> invertible? If
so, find <span class="math inline">\(\textbf{B}^{-1}\)</span>.</p></li>
<li><p>Is <span class="math inline">\(\textbf{B}\)</span>
diagonalizable? If so, find its diagonalization.</p></li>
</ul>
<h1 id="taking-chances-probability-review">Taking Chances: Probability
Review</h1>
<h2 id="basic-probability">Basic probability</h2>
<p>Answer the following questions. You do not need to show your
work.</p>
<ol>
<li><p>You are offered the opportunity to play the following game: your
opponent rolls 2 regular 6-sided dice. If the difference between the two
rolls is at least 3, you win $15. Otherwise, you get nothing. What is a
fair price for a ticket to play this game once? In other words, what is
the expected value of playing the game?</p></li>
<li><p>Consider two events <span class="math inline">\(A\)</span> and
<span class="math inline">\(B\)</span> such that <span
class="math inline">\(\mathbf{P}(A, B)=0\)</span> (they are mutually
exclusive). If <span class="math inline">\(\mathbf{P}(A) = 0.4\)</span>
and <span class="math inline">\(\mathbf{P}(A \cup B) = 0.95\)</span>,
what is <span class="math inline">\(\mathbf{P}(B)\)</span>? Note: <span
class="math inline">\(p(A, B)\)</span> means “probability of <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span>” while <span class="math inline">\(p(A
\cup B)\)</span> means “probability of <span
class="math inline">\(A\)</span> or <span
class="math inline">\(B\)</span>”. It may be helpful to draw a Venn
diagram.</p></li>
<li><p>Instead of assuming that <span class="math inline">\(A\)</span>
and <span class="math inline">\(B\)</span> are mutually exclusive (<span
class="math inline">\(\mathbf{P}(A,B) = 0)\)</span>, what is the answer
to the previous question if we assume that <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are independent?</p></li>
</ol>
<h2 id="expectations-and-variance">Expectations and Variance</h2>
<p>Suppose we have two coins. Coin <span
class="math inline">\(C_1\)</span> comes up heads with probability <span
class="math inline">\(0.3\)</span> and coin <span
class="math inline">\(C_2\)</span> comes up heads with probability <span
class="math inline">\(0.9\)</span>. We repeat this process 3 times:</p>
<ul>
<li><p>Choose a coin with equal probability.</p></li>
<li><p>Flip that coin once.</p></li>
</ul>
<p>Suppose <span class="math inline">\(X\)</span> is the number of heads
after 3 flips.</p>
<ol>
<li><p>What is <span class="math inline">\(\mathbb{E}\left[ X
\right]\)</span>?</p></li>
<li><p>What is <span class="math inline">\(\textbf{Var}\left[ X
\right]\)</span>?</p></li>
<li><p>Based on the number of heads we get, we earn <span
class="math inline">\(Y = \frac{1}{2 + X}\)</span> dollars. What is
<span class="math inline">\(\mathbb{E}\left[ Y
\right]\)</span>?</p></li>
</ol>
<h2 id="a-variance-paradox">A Variance Paradox?</h2>
<p>For independent identically distributed (i.i.d.) random variables
<span class="math inline">\(X_1, ..., X_n\)</span>, each with
distribution <span class="math inline">\(F\)</span> and variance <span
class="math inline">\(\sigma^2\)</span>. We know that <span
class="math inline">\(\textbf{Var}\left[ X_1 + ... + X_n \right] =
n\sigma^2\)</span>. On the other hand, if <span class="math inline">\(X
\sim F\)</span>, then <span class="math inline">\(\textbf{Var}\left[ X +
X \right] = \textbf{Var}\left[ 2X \right] = 4 \sigma^2\)</span>. Is
there a contradiction here? Explain.</p>
<h1 id="calculus-review">Calculus Review </h1>
<h2 id="one-variable-derivatives">One-variable derivatives</h2>
<p>Answer the following questions. You do not need to show your
work.</p>
<ol>
<li><p>Find the derivative of the function <span
class="math inline">\(f(x) = 3x^2 -2x + 5\)</span>.</p></li>
<li><p>Find the derivative of the function <span
class="math inline">\(f(x) = x(1-x)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(p(x) =
\frac{1}{1+\exp(-x)}\)</span> for <span class="math inline">\(x \in
\mathbb{R}\)</span>. Compute the derivative of the function <span
class="math inline">\(f(x) = x-\log(p(x))\)</span> and simplify it by
using the function <span class="math inline">\(p(x)\)</span>.</p></li>
</ol>
<p>Note that in this course we will use <span
class="math inline">\(\log(x)\)</span> to mean the “natural” logarithm
of <span class="math inline">\(x\)</span>, so that <span
class="math inline">\(\log(\exp(1)) = 1\)</span>. Also, observe that
<span class="math inline">\(p(x) = 1-p(-x)\)</span> for the final
part.</p>
<h2 id="multi-variable-derivative">Multi-variable derivative</h2>
<p>Compute the gradient <span class="math inline">\(\nabla
f(\mathbf{x})\)</span> of each of the following functions. You do not
need to show your work.</p>
<ol>
<li><p><span class="math inline">\(f(\mathbf{x}) = x_1^2 +
\exp(x_2)\)</span> where <span class="math inline">\(\mathbf{x}= [x_1,
x_2] \in \mathbb{R}^2\)</span>.</p></li>
<li><p><span class="math inline">\(f(\mathbf{x}) = \exp(x_1 +
x_2x_3)\)</span> where <span class="math inline">\(\mathbf{x}= [x_1,
x_2, x_3] \in \mathbb{R}^3\)</span>.</p></li>
<li><p><span class="math inline">\(f(\mathbf{x}) = \mathbf{a}^\top
\mathbf{x}\)</span> where <span class="math inline">\(\mathbf{x}\in
\mathbb{R}^2\)</span> and <span class="math inline">\(\mathbf{a} \in
\mathbb{R}^2\)</span>.</p></li>
<li><p><span class="math inline">\(f(\mathbf{x}) = \mathbf{x}^\top A
x\)</span> where <span class="math inline">\(A=\left[ \begin{array}{cc}
        2 &amp; -1 \\
        -1 &amp; 1
    \end{array} \right]\)</span> and <span
class="math inline">\(\mathbf{x}\in \mathbb{R}^2\)</span>.</p></li>
<li><p><span class="math inline">\(f(\mathbf{x}) =
\frac{1}{2}\left\|\mathbf{x} \right\|^2\)</span> where <span
class="math inline">\(\mathbf{x}\in \mathbb{R}^d\)</span>.</p></li>
</ol>
<p>Hint: it is helpful to write out the linear algebra expressions in
terms of summations.</p>
<h1 id="algorithms-and-data-structures-review">Algorithms and Data
Structures Review</h1>
<p>For these questions you may find it helpful to review <a
href="http://www.cs.ubc.ca/~schmidtm/Courses/Notes/bigO.pdf">these
notes</a> or <a href="https://en.wikipedia.org/wiki/Big_O_notation">this
Wiki page</a> on big-O notation. Now, answer the following questions
using big-<span class="math inline">\(O\)</span> notation You do not
need to show your work.</p>
<ol>
<li><p>What is the cost of running the merge-sort algorithm to sort a
list of <span class="math inline">\(n\)</span> numbers?</p></li>
<li><p>What is the cost of finding the third-largest element of an
unsorted list of <span class="math inline">\(n\)</span>
numbers?</p></li>
<li><p>What is the cost of finding the smallest element greater than 0
in a <em>sorted</em> list with <span class="math inline">\(n\)</span>
numbers?</p></li>
<li><p>What is the cost of finding the value associated with a key in a
hash table with <span class="math inline">\(n\)</span> numbers?<br />
(Assume the values and keys are both scalars.)</p></li>
<li><p>What is the cost of computing the matrix-vector product <span
class="math inline">\(A\mathbf{x}\)</span> when <span
class="math inline">\(A\)</span> is <span class="math inline">\(n \times
d\)</span> and <span class="math inline">\(x\)</span> is <span
class="math inline">\(d \times 1\)</span>?</p></li>
<li><p>What is the cost of computing the quadratic form <span
class="math inline">\(\mathbf{x}^\top A\mathbf{x}\)</span> when <span
class="math inline">\(A\)</span> is <span class="math inline">\(d \times
d\)</span> and <span class="math inline">\(\mathbf{x}\)</span> is <span
class="math inline">\(d \times 1\)</span>?</p></li>
<li><p>What is the cost of computing matrix multiplication <span
class="math inline">\(AB\)</span> when <span
class="math inline">\(A\)</span> is <span class="math inline">\(m \times
n\)</span> and <span class="math inline">\(B\)</span> is <span
class="math inline">\(n \times d\)</span>?</p></li>
</ol>
<h1 id="programming">Programming</h1>
<p>In this programming homework, we will</p>
<ul>
<li><p>get familiar with PyTorch and its basics.</p></li>
<li><p>build simple text classifiers with Pytorch for sentiment
classification.</p></li>
<li><p>explore different word representational choices (i.e. pre-trained
word embeddings) and their effects on the performance of the
classifiers.</p></li>
</ul>
<h4 id="skeleton-code-and-structure">Skeleton Code and Structure:</h4>
<p>The code base for this homework can be found through our Canvas page.
Your task is to fill in the missing parts in the skeleton code,
following the requirements, guidance, and tips provided in this pdf and
the comments in the corresponding .py files. The code base has the
following structure:</p>
<ul>
<li><p><code>basics.py</code> introduces and demonstrates the usage of
PyTorch basics, e.g. tensors, tensor operations, etc.</p></li>
<li><p><code>model.py</code> implements a sentiment classifier on movie
reviews from scratch with PyTorch.</p></li>
<li><p><code>main.py</code> provides the entry point to run your
implementations in both <code>basics.py</code> and
<code>model.py</code>.</p></li>
<li><p><code>hw1.md</code> provides instructions on how to setup the
environment and run each part of the homework in
<code>main.py</code></p></li>
</ul>
<p><span style="color: blue"><strong>TODOs</strong></span> — Many parts
of this homework involve simply understanding and running the code
already provided in the skeleton, while there is a subset of tasks where
you need to 1) generate plots and write short answers based on the
results of running the code; 2) fill in the blanks in the skeleton to
complete the pipeline. We will explicitly mark these plotting, written
answer, and filling-in-the-blank tasks as <span
style="color: blue"><strong>TODOs</strong></span> in the following
descriptions, as well as a <span
style="color: blue"><strong><code># TODO</code></strong></span> at the
corresponding blank in the code.</p>
<h4 id="submission">Submission:</h4>
<p>Your submission should contain two parts: 1) plots and short answers
under the corresponding questions below; and 2) your completion of the
skeleton code base, in a <code>.zip</code> file</p>
<h2 id="pytorch-basics">PyTorch Basics</h2>
<p>Throughout this course, we will explore several interesting
programming problems where you will gain hands-on experience by
implementing the concepts/methods/models learned in the lectures. Many
of the implementations will be based on the <code>PyTorch</code>
framework.</p>
<p><a href="https://pytorch.org/">PyTorch</a> is an open-source machine
learning library for Python. It is widely used for applications such as
natural language processing, computer vision, etc. It was initially
developed by the Facebook artificial intelligence research group (FAIR).
PyTorch redesigns and implements Torch in Python while sharing the same
core C libraries for the backend code. PyTorch developers tuned this
back-end code to run Python efficiently.</p>
<h3 id="why-pytorch">Why PyTorch?</h3>
<ul>
<li><p>Easy interface: PyTorch offers easy-to-use API. It is easy to
understand and debug the code.</p></li>
<li><p>Python usage: This library is considered to be Pythonic which
smoothly integrates with the Python data science stack.</p></li>
<li><p>Computational graphs and automatic differentiation: will be
covered in later lectures/homework.</p></li>
</ul>
<p>In the first part of this programming homework, we will learn about
some fundamental components of PyTorch, its core representation
(tensor), and its operations.</p>
<h3 id="tensors">Tensors</h3>
<p>A PyTorch tensor (<code>torch.Tensor</code>) is a multi-dimensional
matrix containing elements of a single data type. They are just like
numpy arrays, but they can run on GPU and allow automatic
differentiation. We first create a few PyTorch tensors to work with.
There are multiple ways to create and initialize PyTorch tensors – from
a list or NumPy array, or with some PyTorch functions.</p>
<p>Read and run the <code>tensor_creation</code> function in
<code>basic.py</code>, which introduces multiple ways of tensor
creation, data type and shape.</p>
<h3 id="tensor-operations">Tensor Operations</h3>
<p>Similar to how you deal with arrays in <code>Numpy</code>, most of
the operations that exisit in numpy, also exist in PyTorch. They also
share a very similar interface (<a
href="https://numpy.org/devdocs/user/quickstart.html">[a NumPy
tutorial]</a>)</p>
<p>Read and run the <code>tensor_operations</code> function in
<code>basic.py</code>, which detailed several key tensor operations.</p>
<h3 id="mathematical-operations">Mathematical Operations</h3>
<p>Other commonly used operations include matrix multiplications, which
are essential for neural networks. Quite often, we have an input vector
<span class="math inline">\(\mathbf{x}\)</span>, which is transformed
using a learned weight matrix <span
class="math inline">\(\mathbf{W}\)</span>. There are multiple ways and
functions to perform matrix multiplication, some of which we list
below:</p>
<ul>
<li><p>Element-wise sum: <a
href="https://pytorch.org/docs/stable/generated/torch.add.html"><code>torch.add()</code></a></p></li>
<li><p>Element-wise multiplication: <a
href="https://pytorch.org/docs/stable/generated/torch.mul.html"><code>torch.mul()</code></a></p></li>
</ul>
<p>Instead of explicitly invoking PyTorch functions, we may use <a
href="https://docs.python.org/3/library/operator.html#mapping-operators-to-functions">built-in
operators</a> in Python. For example, given two PyTorch tensors
<code>a</code> and <code>b</code>, <code>torch.add(a, b)</code> is
equivalent to <code>a + b</code>.</p>
<p>Read and run the <code>math_operations</code> function in
<code>basic.py</code>, which detailed several key math operations.</p>
<h3 id="pytorch-and-numpy-bridge">PyTorch and NumPy Bridge</h3>
<p>It is also very convenient to convert PyTorch tensors to NumPy
arrays, and vice versa.</p>
<ul>
<li><p>By using <code>.numpy()</code> on a tensor, we can easily convert
tensor to <code>ndarray</code>.</p></li>
<li><p>To convert NumPy <code>ndarray</code> to PyTorch tensor, we can
use <code>.from_numpy()</code> to convert ndarray to tensor</p></li>
</ul>
<p>Read and run the <code>torch_numpy</code> function in
<code>basic.py</code>, which detailed the torch-numpy conversions.</p>
<h2
id="sentiment-classification-with-pytorch-and-word-embeddings">Sentiment
Classification with PyTorch and Word Embeddings</h2>
<p>In the second part of the programming homework, we will build a
simple sentiment classifier using PyTorch, with additional different
word embeddings. We will use the <a
href="https://huggingface.co/datasets/imdb">IMDB dataset</a>, which has
reviews about movies that are manually annotated with binary positive
(label = 1) or negative reviews (label = 0).</p>
<p>Spend a few minutes reading a few examples on Huggingface to get a
better sense of what this dataset looks like. We will use Huggingface’s
datasets library to download this dataset locally.</p>
<h3 id="subsubsec: data loading and splits">Data Loading and Splits</h3>
<p>The training set is used to train the model while the test set is
used to evaluate the model’s performance. Since we don’t want to overfit
the test set, we will not evaluate on it more than just a few times when
we are done with model training. This is very important!!</p>
<p>We will also set aside a subset of the training set as the
development set. Dev sets are used in machine learning to evaluate the
model’s performance during the training process, providing an
intermediate check on the model’s accuracy before it is evaluated on the
test set.</p>
<p>Dev sets prevent overfitting during training. Overfitting occurs when
a model is too complex and fits the training data too well, leading to
poor performance generalization on new data. The development set allows
for monitoring of the model’s performance on data it has not seen during
training, helping to avoid overfitting. We will also cap our train, dev,
and test sets at 20k, 1k, and 1k to make our training/evaluation faster,
obviously at the cost of a less accuracy.</p>
<p>Read the <code>load_data</code> function in <code>model.py</code> to
get an understanding of how to download, sub-select and split the raw
data.</p>
<h3 id="word-embeddings-representing-meaning-in-a-computer">Word
Embeddings: Representing Meaning in a Computer</h3>
<p>While we can easily read and understand these movie reviews, they
make no sense to a computer as mere sequences of strings. How can we
represent the meaning of texts, i.e. <em>semantics</em> (roughly
speaking), in computers so that it "makes sense" computationally?</p>
<p>A traditional approach is to regard words as <em>discrete
symbols</em>. We first compile a list of unique words (e.g. <span
class="math inline">\(V\)</span> = 50,000 top frequent English words) as
vocabulary, then each word can be represented as an <em>one-hot</em>
vector of dimension <span class="math inline">\(V\)</span>: one <span
class="math inline">\(1\)</span> at the entry corresponding to the index
of that word in the vocabulary, and <span
class="math inline">\(0\)</span>s at all other entries. However, the key
problem of this approach is that it fails to encode some important
aspects of meaning (e.g. similarity) computationally. For example, we
know that “hotel” should be more similar to “motel” than to “apple”, but
their one-hot representations are mutually orthogonal with distances all
equal to <span class="math inline">\(\sqrt{2}\)</span> - we can not tell
the differences!</p>
<p>An alternative approach, which marks one of the most successful and
important milestones of modern statistical NLP, is <em>Distributional
Semantics</em> <span class="citation" data-cites="Firth1957ASO"></span>.
The key idea is that <em>“You shall know a word by the company it
keeps”</em> - A word’s meaning is given by the words that frequently
appear close by. Under this notion, each word is represented by a
<em>dense vector</em>, chosen so that it is similar to vectors of words
that appear in similar contexts, where similarity is measured by the
vector dot product. Note that word vectors are also called <em>(word)
embeddings</em>, which we will be mostly referring to in this and the
following homework. The most widely adopted frameworks for obtaining
word embeddings are learning-based methods that focus on word
co-occurrence patterns in local context windows, e.g. Word2vec <span
class="citation" data-cites="Mikolov2013DistributedRO"></span>, or
global co-occurrence statistics, e.g. GloVe <span class="citation"
data-cites="pennington2014glove"></span>. And it has been shown that
such learned word embeddings have succeeded in capturing fine-grained
semantic and syntactic patterns with vector arithmetic, and are
beneficial to many downstream NLP tasks. We refer you to the <a
href="https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture01-wordvecs1-public.pdf">Stanford
CS 224N Slides 1</a>, <a
href="https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture02-wordvecs2.pdf">Stanford
CS 224N Slides 2</a> and the cited papers for more details about meaning
representations and word embeddings.</p>
<h3
id="string-to-feature-featurizing-input-text-with-word-embeddings">String
to Feature: Featurizing Input Text with Word Embeddings</h3>
<p>Given the powerful representation encoded in word vectors, we will
use some pre-trained word embeddings to represent movie reviews as input
features to our classifier. Specifically, We will convert each input
review into a continuous feature vector. To do so, we will first
<em>tokenize</em> each input sentence into a sequence of tokens, and map
each token to the corresponding word vector. Finally, we take the
average over all the word embeddings of that review to represent its
“semantic” feature.</p>
<p>In this homework, we leverage several pre-trained embeddings provided
in <a
href="https://github.com/piskvorky/gensim#documentation"><code>Gensim</code></a>:
a Python library for topic modeling, document indexing and similarity
retrieval with large corpora. As you will see in the code base, each
<code>Gensim</code> embeddings is a KeyedVectors that stores embeddings
of the vocabulary as a numpy <code>ndarray</code> with shape
[<code>vocab_size</code>, <code>embed_size</code>], and it supports
direct string-based access, e.g. <code>embeddings[“hotel”]</code> will
return the word vector of “hotel”.<br />
<br />
<span style="color: blue"><strong>TODOs</strong></span>: read and
complete the missing lines in the <code>featurize</code> function in
<code>model.py</code>, which converts an input string into a tensor
following the description above and the comments in the code.<br />
<strong>Hint</strong>: You can refer to the Pytorch NumPy Bridge and
<code>torch_numpy</code> discussed above for converting numpy arrays to
tensors.</p>
<h3 id="dataset-and-dataloader">Dataset and Dataloader</h3>
<p>PyTorch has two primitives to work with data:
<code>torch.utils.data.Dataset</code> and
<code>torch.utils.data.DataLoader</code> (<a
href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">tutorial</a>).
<code>Dataset</code> stores each data sample and corresponding
labels/auxiliary information and allows us to use pre-loaded/customized
data. <code>DataLoader</code> wraps an iterable around the
<code>Dataset</code> to enable easy controllable/randomized access to
the subset (mini-batch) of samples.<br />
We will first apply the featurization function we just completed to all
the samples in the raw data, stack the feature tensors and labels into
two single tensors to create a <a
href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset"><code>TensorDataset</code></a>.<br />
<br />
<span style="color: blue"><strong>TODOs</strong></span>: read and
complete the missing lines in the <code>create_tensor_dataset</code>
function in <code>model.py</code>, which converts an input string into a
tensor following the description above and the comments in the
code.<br />
Then we will use the <code>create_dataloader</code> function in
<code>model.py</code> to wrap each dataset with a dataloader.</p>
<h3 id="defining-our-first-pytorch-model-nn.module">Defining our First
PyTorch Model: <code>nn.Module</code></h3>
<p>Now that we have finished data processing and loading, it is time to
build the model! In PyTorch, a neural network is built up out of
modules. Specifically, a model is represented by a regular Python class
that inherits from the <a
href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">torch.nn.Module</a>.
Modules can contain other modules, and a neural network is considered to
be a module itself as well.</p>
<p>The are two most important components in <code>torch.nn.Module</code>
are</p>
<ul>
<li><p><code>__init__(self)</code> where we define the model
parts</p></li>
<li><p><code>forward(self, x)</code> where the forward inference
happens</p></li>
</ul>
<p>The basic template of a module is as follows:</p>
<div class="sourceCode" id="cb1" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyModule(nn.Module):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Some init for my module</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Function for performing the calculation of the module.</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div>
<p>The forward function is where the computation of the module takes
place, and is executed when you call the module
(<code>nn = MyModule(); nn(x)</code>).</p>
<p>There are a few important properties of
<code>torch.nn.Module</code>:</p>
<ul>
<li><p><code>state_dict()</code> which returns a dictionary of the
trainable parameters with their current values</p></li>
<li><p><code>parameters()</code> which returns a list of all trainable
parameters that are used in the forward function.</p></li>
<li><p><code>train()</code> or <code>eval()</code> that makes the model
trainable (or fixed) for training (or evaluation) purposes</p></li>
</ul>
<p>Note, the backward calculation is done automatically but could be
overwritten as well if wanted.<br />
<br />
For this homework, we will build a sentiment classifier that consists
of</p>
<ul>
<li><p><code>nn.Linear</code> layer that projects the average embedding
vector of each sequence to a c-dimension vector, represents the
real-valued score for each label class (c <span
class="math inline">\(=2\)</span>) in our case.</p></li>
<li><p><code>nn.CrossEntropyLoss</code> that normalizes the real-valued
scores into probability distribution and calculates the cross-entropy
loss with the ground truth (binary <span
class="math inline">\(0\)</span>-<span class="math inline">\(1\)</span>)
distribution</p></li>
</ul>
<p><span style="color: blue"><strong>TODOs</strong></span>: read and
complete the missing lines in the <code>__init__</code> and
<code>forward</code> function of the <code>SentimentClassifier</code>
class in <code>model.py</code>, to create an linear layer and perform
forward pass. <strong>Hint</strong>: check out <a
href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code>nn.Linear</code></a>
for the definition and forward usage of the linear layer.</p>
<h3 id="chain-everything-together-training-and-evaluation">Chain
Everything Together: Training and Evaluation</h3>
<p>As we have all the components ready, we can chain them together to
build the training and evaluation pipeline. A common training pipeline
usually involves:</p>
<ul>
<li><p>Data loading</p></li>
<li><p>Model initialization and/or weights loading</p></li>
<li><p>Training loop of forward pass, backward pass, loss calculation,
and gradient updates</p></li>
<li><p>Evaluation</p></li>
</ul>
<p><span style="color: blue"><strong>TODOs</strong></span>: read and
complete the missing lines in the <code>accuracy</code> function in
<code>model.py</code>, to compute the accuracy of model
predictions.<br />
<strong>Hint</strong>: your return should be a tensor of <span
class="math inline">\(0\)</span>s and <span
class="math inline">\(1\)</span>s, indicating the correctness of each
prediction. Remember that the prediction (logits) tensor has the shape
of <span class="math inline">\([\text{batch\_size}, \,
\text{num\_classes}]\)</span>, check out <a
href="https://pytorch.org/docs/stable/generated/torch.argmax.html">torch.argmax</a>
for selecting the indices of the maximum value along certain
dimension.<br />
<br />
Then, read <code>train</code> and <code>evaluate</code> function in
<code>model.py</code> that provides a simple demonstration of the
training/evaluation pipeline.</p>
<h3 id="run-the-pipeline-train-loss-vs.-dev-loss">Run the pipeline:
Train Loss vs. Dev Loss</h3>
<p>Once you have completed all the <span
style="color: blue"><strong>TODOs</strong></span> above, you can run the
pipeline to train and evaluate our model. We have provided a
visualization function <code>visualize_epochs</code> in
<code>model.py</code> to track and plot the model performance (loss on
train and dev set) along the training progress.<br />
<br />
<span style="color: blue"><strong>TODOs</strong></span>: run the
<code>single_run</code> function in <code>main.py</code>, paste the plot
here, and describe in 2-3 sentences your findings.<br />
<strong>Hint</strong>: Do you observe any discrepancy between the trend
of train loss and dev loss? What it might indicate?<br />
<br />
<br />
</p>
<h3 id="run-the-pipeline-explore-different-word-embeddings">Run the
pipeline: Explore Different Word Embeddings</h3>
<p>As discussed earlier, we initialize the embedding layer of the
classifier with pre-trained word embeddings. We have provided in
<code>main.py</code> 4 different types of pre-trained word embeddings as
different representational options for you to explore their effects on
model performance. Again, we provide a visualization function
<code>visualize_configs</code> to depict the performance (dev loss and
dev accurracy) across model configurations with different embedding
choices.<br />
<br />
<span style="color: blue"><strong>TODOs</strong></span>: run the
<code>explore_embeddings</code> function in <code>main.py</code>, paste
the two plots here, and describe in 2-3 sentences your findings.<br />
<strong>Hint</strong>: Do you observe any performance differences across
different embeddings? What might be the reason of such
differences?<br />
<br />
<br />
</p>
<h1 class="unnumbered" id="optional-feedback">Optional Feedback</h1>
<p>Have feedback for this assignment? Found something confusing? We’d
love to hear from you!</p>
</body>
</html>
